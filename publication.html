<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Yi Zhang</title>
    <base href="https://uyzhang.github.io/publication.html">
</head>

<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<h1 style="padding-left: 0.5em">Yi Zhang (Ph.D candidate at Beihang University)</h1><hr>
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="publication.html">Publications</a></div>
</td>
<td id="layout-content">

     <h1 style="margin-top: 0em">Publications</a></h1><br>
    <p>For details, please refer to google scholar.</p>

    <!-- <div>
        <h2><hr>Preprints</h2>
        <ul>
            <li><p>
                Agentic Keyframe Search for Video Question Answering <br>
                Sunqi Fan, <b>Meng-Hao Guo</b>, Shuojin Yang<br>
                [<A HREF="https://arxiv.org/pdf/2503.16032">PDF</A>]
            </p></li>
        </ul>
    </div> -->

    <div>
        <h2><hr><a name="conference"></a>Conference Papers</h2>
        <ol>
            <li><p>
                Adaptive Parameter Selection for Tuning Vision-Language Models. CVPR 2025 <br>
                <b>Yi Zhang</b>, Yi-Xuan Deng, Meng-Hao Guo, Shi-Min Hu <br>
                [<A HREF="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Adaptive_Parameter_Selection_for_Tuning_Vision-Language_Models_CVPR_2025_paper.pdf">PDF</A>]
            </p></li>
            <li><p>
                Exploring regional clues in CLIP for zero-shot semantic segmentation. CVPR 2024 <br>
                <b>Yi Zhang</b>, Meng-Hao Guo, Miao Wang, Shi-Min Hu <br>
                [<A HREF="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Regional_Clues_in_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2024_paper.pdf">PDF</A>]
            </p></li>
            <li><p>
                RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs. NeurIPS 2025 <br>
                Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-Lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, <b>Yi Zhang</b>, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-Min Hu <br>
                [<A HREF="https://arxiv.org/pdf/2505.16770">PDF</A>]
            </p></li>
            <li><p>
                R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation. ICML 2025 <br>
                Meng-Hao Guo, Jiajun Xu, <b>Yi Zhang</b>, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu <br>
                [<A HREF="https://arxiv.org/pdf/2505.02018">PDF</A>]
            </p></li>
        </ol>
    </div>
    
    <div>
        <h2><hr><a name="journal"></a>Journal Papers</h2>
        <ol>
            <li><p>
                Tuning Vision-Language Models With Multiple Prototypes Clustering. T-PAMI 2024 <br>
                Meng-Hao Guo, <b>Yi Zhang</b>, Tai-Jiang Mu, Sharon X Huang, Shi-Min Hu.<br>
                [<a href="https://cg.cs.tsinghua.edu.cn/papers/PAMI-2024-TuningVLM.pdf" target="_blank">PDF</a>]
            </p></li>
        </ol>
    </div>
</td>
</tr>
</table>
</body>
</html>
